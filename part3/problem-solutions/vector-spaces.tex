\section{Vector Spaces}
\begin{questions}
    \item \begin{partquestions}{\alph*}
        \item Since $\textbf{0} \in U$ (as $U$ is a subspace of $V$) and since $\textbf{0} \in W$ (as $W$ is a subspace of $V$), therefore $\textbf{0} \in U \cap W$, meaning $U \cap W$ is non-empty.
        
        Now suppose $\textbf{u}, \textbf{v} \in U \cap W$. So $\textbf{u}, \textbf{v} \in U$ and $\textbf{u}, \textbf{v} \in W$. Hence $\textbf{u} + \textbf{v} \in U$ and $\textbf{u} + \textbf{v} \in W$ by closure of vector spaces by addition. Therefore $\textbf{u} + \textbf{v} \in U \cap W$.

        Now let $\alpha \in F$ and $\textbf{u} \in U \cap W$. So $\textbf{u} \in U$ and $\textbf{u} \in W$. This means $\alpha\textbf{u} \in U$ and $\alpha\textbf{u} \in W$ by closure of vector spaces by scalar multiplication. Hence $\alpha\textbf{u} \in U \cap W$.

        So we see $U \cap W$ is a subspace of $V$ by the subspace test (\myref{thrm-subspace-test}).

        \item Note that since $\textbf{0} \in U$ and $\textbf{0} \in W$, we see $\textbf{0} = \textbf{0} + \textbf{0} \in U + W$, meaning $U + W$ is non-empty.
        
        Now suppose $\textbf{a}, \textbf{b} \in U + W$, meaning $\textbf{a} = \textbf{u}_1 + \textbf{w}_1$ and $\textbf{b} = \textbf{u}_2 + \textbf{w}_2$ for some $\textbf{u}_1, \textbf{u}_2 \in U$ and $\textbf{w}_1, \textbf{w}_2 \in W$. Hence one sees $\textbf{u}_1 + \textbf{u}_2 \in U$ and $\textbf{w}_1 + \textbf{w}_2 \in W$, meaning
        \begin{align*}
            \textbf{a} + \textbf{b} &= (\textbf{u}_1 + \textbf{w}_1) + (\textbf{u}_2 + \textbf{w}_2)\\
            &= (\textbf{u}_1 + \textbf{u}_2) + (\textbf{w}_1 + \textbf{w}_2)\\
            &\in U + W.
        \end{align*}
        Also, for any $\alpha \in F$ we see $\alpha\textbf{u}_1 \in U$ and $\alpha\textbf{w}_1 \in W$, which means
        \begin{align*}
            \alpha\textbf{a} &= \alpha(\textbf{u}_1 + \textbf{w}_1)\\
            &= (\alpha\textbf{u}_1) + (\alpha\textbf{w}_1)\\
            &\in U + W.
        \end{align*}
        Therefore $U + W$ is a subspace of $V$ by the subspace test (\myref{thrm-subspace-test}).
    \end{partquestions}

    \item Consider $S = \{\textbf{u}_1, \textbf{u}_2, \textbf{u}_3\} = \{\textbf{v}_1, \textbf{v}_1 + \textbf{v}_2, \textbf{v}_1 + \textbf{v}_2 + \textbf{v}_3\}$. Since $\{\textbf{v}_1,\textbf{v}_2,\textbf{v}_3\}$ is a basis for $V$, meaning that any $\textbf{a} \in V$ can be expressed as
    \[
        \alpha_1\textbf{v}_1 + \alpha_2\textbf{v}_2 + \alpha_3\textbf{v}_3
    \]
    where $\alpha_1, \alpha_2, \alpha_3 \in F$. One therefore sees
    \begin{align*}
        \textbf{a} &= \alpha_1\textbf{v}_1 + \alpha_2\textbf{v}_2 + \alpha_3\textbf{v}_3\\
        &= (\alpha_1 - \alpha_2)\textbf{v}_1 + (\alpha_2 - \alpha_3)(\textbf{v}_1 + \textbf{v}_2) + \alpha_3(\textbf{v}_1 + \textbf{v}_2 + \textbf{v}_3)\\
        &= (\alpha_1 - \alpha_2)\textbf{u}_1 + (\alpha_2 - \alpha_3)\textbf{u}_2 + \alpha_3\textbf{u}_3
    \end{align*}
    which means $\Span{S} = V$.

    Now we show that the vectors in $S$ are linearly independent. Suppose that
    \[
        \beta_1\textbf{u}_1 + \beta_2\textbf{u}_2 + \beta_3\textbf{u}_3 = \textbf{0}
    \]
    for some $\beta_1, \beta_2, \beta_3 \in F$. This means
    \begin{align*}
        \beta_1\textbf{u}_1 + \beta_2\textbf{u}_2 + \beta_3\textbf{u}_3 &= \beta_1\textbf{v}_1 + \beta_2(\textbf{v}_1 + \textbf{v}_2) + \beta_3(\textbf{v}_1 + \textbf{v}_2 + \textbf{v}_3)\\
        &= (\beta_1+\beta_2+\beta_3)\textbf{v}_1 + (\beta_2+\beta_3)\textbf{v}_2 + \beta_3\textbf{v}_3\\
        &= \textbf{0}.
    \end{align*}
    Since $\{\textbf{v}_1, \textbf{v}_2, \textbf{v}_3\}$ is a basis, the only way for this to occur is when
    \begin{align*}
        \beta_1 + \beta_2 + \beta_3 &= 0,\\
        \beta_2 + \beta_3 &= 0, \text{ and}\\
        \beta_3 & = 0,
    \end{align*}
    which clearly means $\beta_1 = \beta_2 = \beta_3 = 0$. Therefore the vectors in $S$ are linearly independent, which shows that $S$ is a basis for $V$.
    
    \item \begin{partquestions}{\alph*}
        \item Note $(1, 0, 0, 0), (0, 1, 0, 0) \in R$ (since $1 + 0 + 0 + 0 = 1$ and $0 + 1 + 0 + 0 = 1$) but
        \[
            (1, 0, 0, 0) + (0, 1, 0, 0) = (1, 1, 0, 0)
        \]
        is not since $1 + 1 + 0 + 0 = 2 \neq 1$. Therefore $R$ is not closed under vector addition, meaning that it is not a vector space (and hence not a subspace).
        
        \item We first show that $S$ is a subspace of $V$.
        
        Note $(0, 0, 0, 0) \in S$ since $0 + 0 + 0 + 0 = 0$, which means $S$ is non-empty.

        Now suppose $(a_1, a_2, a_3, a_4), (b_1, b_2, b_3, b_4) \in V$. So
        \begin{align*}
            a_1 + a_2 + a_3 + a_4 &= 0\\
            b_1 + b_2 + b_3 + b_4 &= 0
        \end{align*}
        which therefore means
        \begin{align*}
            &(a_1 + b_1) + (a_2 + b_2) + (a_3 + b_3) + (a_4 + b_4)\\
            &= (a_1 + a_2 + a_3 + a_4) + (b_1 + b_2 + b_3 + b_4)\\
            &= 0
        \end{align*}
        Hence
        \[
            (a_1, a_2, a_3, a_4) + (b_1, b_2, b_3, b_4) = (a_1 + b_1, a_2 + b_2, a_3 + b_3, a_4 + b_4) \in S.
        \]

        Finally suppose $\alpha \in \R$ and $(a_1, a_2, a_3, a_4) \in V$. One sees that
        \[
            \alpha a_1 + \alpha a_2 + \alpha a_3 + \alpha a_4 = \alpha(a_1 + a_2 + a_3 + a_4) = 0
        \]
        and so
        \[
            \alpha(a_1, a_2, a_3, a_4) = (\alpha a_1, \alpha a_2 , \alpha a_3, \alpha a_4) \in S.
        \]

        Therefore, by subspace test (\myref{thrm-subspace-test}), we see $S$ is a subspace of $V$.

        Now note that any $(a, b, c, d) \in S$ is expressible in the form
        \[
            (a, b, c, -a-b-c) = a(1, 0, 0, -1) + b(0, 1, 0, -1) + c(0, 0, 1, -1)
        \]
        and so the vectors in the set $A = \{(1, 0, 0, -1), (0, 1, 0, -1), (0, 0, 1, -1)\}$ span $V$. We also see that if
        \[
            \alpha(1, 0, 0, -1) + \beta(0, 1, 0, -1) + \gamma(0, 0, 1, -1) = (0, 0, 0, 0)
        \]
        we must have
        \begin{align*}
            \alpha &= 0\\
            \beta &= 0\\
            \gamma &= 0\\
            -\alpha - \beta - \gamma &= 0
        \end{align*}
        and so the only solution to the above equation is the trivial one, where $\alpha = \beta = \gamma = 0$. This shows that the vectors in $A$ are linearly independent.

        Therefore, $A$ is a basis for $S$. Since $|A| = 3$, therefore $S$ has dimension 3, i.e. $\dim{S} = 3$.
    \end{partquestions}
    
    \item We are to prove the vector space axioms for $F^n$ over $F$.
    \begin{itemize}
        \item \textbf{Addition-Abelian}: We are to show that $(F^n, +)$ is an abelian group.
        \begin{itemize}
            \item \textbf{Closure}: From the definition of vector addition it is clear that closure is satisfied.
            
            \item \textbf{Associativity}: For any $(u_1, u_2, \dots, u_n), (v_1, v_2, \dots, v_n), (w_1, w_2, \dots, w_n) \in F^n$ we see
            \begin{align*}
                &(u_1, u_2, \dots, u_n) + ((v_1, v_2, \dots, v_n) + (w_1, w_2, \dots, w_n))\\
                &= (u_1, u_2, \dots, u_n) + (v_1 + w_1, v_2 + w_2, \dots, v_n + w_n)\\
                &= (u_1 + (v_1 + w_1), u_2 + (v_2 + w_2), \dots, u_n + (v_n + w_n))\\
                &= ((u_1 + v_1) + w_1, (u_2 + v_2) + w_2, \dots, (u_n + v_n) + w_n)\\
                &= (u_1 + v_1, u_2 + v_2, \dots, u_n + v_n) + (w_1, w_2, \dots, w_n)\\
                &= ((u_1, u_2, \dots, u_n) + (v_1, v_2, \dots, v_n)) + (w_1, w_2, \dots, w_n)
            \end{align*}
            which proves that vector addition is associative.
            
            \item \textbf{Identity}: $(0, 0, \dots, 0) \in F^n$ is the identity since, for any $(u_1, u_2, \dots, u_n) \in F^n$, we see
            \begin{align*}
                (0, 0, \dots, 0) + (u_1, u_2, \dots, u_n) &= (0 + u_1, 0 + u_2, \dots, 0 + u_n)\\
                &= (u_1, u_2, \dots, u_n).
            \end{align*}
            
            \item \textbf{Inverse}: For any $(u_1, u_2, \dots, u_n) \in F^n$, we note that $(-u_1, -u_2, \dots, -u_n)$ is its additive inverse since
            \begin{align*}
                (u_1, u_2, \dots, u_n) + (-u_1, -u_2, \dots, -u_n) &= (u_1 - u_1, u_2 - u_2, \dots, u_n - u_n)\\
                &= (0, 0, \dots, 0).
            \end{align*}
            
            \item \textbf{Commutativity}: For any $(u_1, u_2, \dots, u_n), (v_1, v_2, \dots, v_n) \in F^n$ we note
            \begin{align*}
                (u_1, u_2, \dots, u_n) + (v_1, v_2, \dots, v_n) &= (u_1 + v_1, u_2 + v_2, \dots, u_n + v_n)\\
                &= (v_1 + u_1, v_2 + u_2, \dots, v_n + u_n)\\
                &= (v_1, v_2, \dots, v_n) + (u_1, u_2, \dots, u_n)
            \end{align*}
            so addition is commutative.
        \end{itemize}
        Therefore $(F^n, +)$ is an abelian group.
        
        \item \textbf{Multiplication-Identity}: Note for any $(u_1, u_2, \dots, u_n) \in F^n$ we have
        \begin{align*}
            1(u_1, u_2, \dots, u_n) &= (1u_1, 1u_2, \dots, 1u_n)\\
            &= (u_1, u_2, \dots, u_n)
        \end{align*}
        so this axiom is satisfied.

        \item \textbf{Multiplication-Compatibility}: Let $\alpha, \beta \in F$ and $(u_1, u_2, \dots, u_n) \in F^n$. Then
        \begin{align*}
            \alpha(\beta(u_1, u_2, \dots, u_n)) &= \alpha(\beta u_1, \beta u_2, \dots, \beta u_n)\\
            &= (\alpha\beta u_1, \alpha\beta u_2, \dots, \alpha\beta u_n)\\
            &= (\alpha\beta)(u_1, u_2, \dots, u_n).
        \end{align*}
        Thus this axiom is satisfied.
        
        \item \textbf{Distributivity-Addition}: Let $\alpha \in F$ and $(u_1, u_2, \dots, u_n), (v_1, v_2, \dots, v_n) \in F^n$. Then
        \begin{align*}
            &\alpha((u_1, u_2, \dots, u_n) + (v_1, v_2, \dots, v_n))\\
            &= \alpha(u_1 + v_1, u_2 + v_2, \dots, u_n + v_n)\\
            &= (\alpha(u_1 + v_1), \alpha(u_2 + v_2), \dots, \alpha(u_n + v_n))\\
            &= (\alpha u_1 + \alpha v_1, \alpha u_2 + \alpha v_2, \dots, \alpha u_n + \alpha v_n)\\
            &= (\alpha u_1, \alpha u_2, \dots, \alpha u_n) + (\alpha v_1, \alpha v_2, \dots, \alpha v_n)\\
            &= \alpha(u_1, u_2, \dots, u_n) + \alpha(v_1, v_2, \dots, v_n)
        \end{align*}
        which shows that the axiom is satisfied.
        
        \item \textbf{Distributivity-Scalar}: Let $\alpha, \beta \in F$ and $(u_1, u_2, \dots, u_n) \in F^n$. Note
        \begin{align*}
            (\alpha+\beta)(u_1, u_2, \dots, u_n) &= ((\alpha+\beta)u_1, (\alpha+\beta)u_2, \dots, (\alpha+\beta)u_n)\\
            &= (\alpha u_1 + \beta u_1, \alpha u_2 + \beta u_2, \dots, \alpha u_n + \beta u_n)\\
            &= (\alpha u_1, \alpha u_2, \dots, \alpha u_n) + (\beta u_1, \beta u_2, \dots, \beta u_n)\\
            &= \alpha(u_1, u_2, \dots, u_n) + \beta(u_1, u_2, \dots, u_n)
        \end{align*}
        so this axiom is satisfied.
    \end{itemize}
    Since all the vector space axioms are satisfied, this finally means that $F^n$ is a vector space over $F$.
    
    \item Suppose $V$ has a basis of $\{\textbf{u}_1, \textbf{u}_2, \dots, \textbf{u}_n\}$. This means any $\textbf{a} \in V$ can be expressed as
    \[
        \alpha_1\textbf{u}_1 + \alpha_2\textbf{u}_2 + \cdots + \alpha_n\textbf{u}_n
    \]
    where $\alpha_1,\alpha_2,\dots,\alpha_n \in F$.

    Consider the map $T: V \to F^n$ given by
    \[
        T( \alpha_1\textbf{u}_1 + \alpha_2\textbf{u}_2 + \cdots + \alpha_n\textbf{u}_n) = (\alpha_1, \alpha_2, \dots, \alpha_n).
    \]
    We first show that $T$ is a linear transformation.
    \begin{itemize}
        \item For all $\alpha_1\textbf{u}_1 + \alpha_2\textbf{u}_2 + \cdots + \alpha_n\textbf{u}_n, \beta_1\textbf{u}_1 + \beta_2\textbf{u}_2 + \cdots + \beta_n\textbf{u}_n \in V$ we see
        \begin{align*}
            &T((\alpha_1\textbf{u}_1 + \alpha_2\textbf{u}_2 + \cdots + \alpha_n\textbf{u}_n) + (\beta_1\textbf{u}_1 + \beta_2\textbf{u}_2 + \cdots + \beta_n\textbf{u}_n))\\
            &= T((\alpha_1 + \beta_1)\textbf{u}_1 + (\alpha_2 + \beta_2)\textbf{u}_2 + \cdots + (\alpha_n + \beta_n)\textbf{u}_n)\\
            &= (\alpha_1 + \beta_1, \alpha_2 + \beta_2, \dots, \alpha_n + \beta_n)\\
            &= (\alpha_1, \alpha_2, \dots, \alpha_n) + (\beta_1, \beta_2, \dots, \beta_n)\\
            &= T(\alpha_1\textbf{u}_1 + \alpha_2\textbf{u}_2 + \cdots + \alpha_n\textbf{u}_n) + T(\beta_1\textbf{u}_1 + \beta_2\textbf{u}_2 + \cdots + \beta_n\textbf{u}_n).
        \end{align*}

        \item For all $\lambda \in F$ and $\alpha_1\textbf{u}_1 + \alpha_2\textbf{u}_2 + \cdots + \alpha_n\textbf{u}_n \in V$ we see
        \begin{align*}
            T(\lambda(\alpha_1\textbf{u}_1 + \alpha_2\textbf{u}_2 + \cdots + \alpha_n\textbf{u}_n)) &= T((\lambda\alpha_1)\textbf{u}_1 + (\lambda\alpha_2)\textbf{u}_2 + \cdots + (\lambda\alpha_n)\textbf{u}_n)\\
            &= (\lambda\alpha_1, \lambda\alpha_2, \dots, \lambda\alpha_n)\\
            &= \lambda(\alpha_1, \alpha_2, \dots, \alpha_n)\\
            &= \lambda T(\alpha_1\textbf{u}_1 + \alpha_2\textbf{u}_2 + \cdots + \alpha_n\textbf{u}_n).
        \end{align*}
    \end{itemize}
    Therefore $T$ is a linear transformation.
    
    We now prove that $T$ is a bijection.
    \begin{itemize}
        \item \textbf{Injective}: Suppose $\alpha_1\textbf{u}_1 + \alpha_2\textbf{u}_2 + \cdots + \alpha_n\textbf{u}_n, \beta_1\textbf{u}_1 + \beta_2\textbf{u}_2 + \cdots + \beta_n\textbf{u}_n \in V$ are such that $T(\alpha_1\textbf{u}_1 + \alpha_2\textbf{u}_2 + \cdots + \alpha_n\textbf{u}_n) = T(\beta_1\textbf{u}_1 + \beta_2\textbf{u}_2 + \cdots + \beta_n\textbf{u}_n)$. This therefore means $(\alpha_1, \alpha_2, \dots, \alpha_n) = (\beta_1, \beta_2, \dots, \beta_n)$, which shows that $\alpha_i = \beta_i$ for all $i \in \{1, 2, \dots, n\}$. It follows quickly from this result that $\alpha_1\textbf{u}_1 + \alpha_2\textbf{u}_2 + \cdots + \alpha_n\textbf{u}_n = \beta_1\textbf{u}_1 + \beta_2\textbf{u}_2 + \cdots + \beta_n\textbf{u}_n$, proving that $T$ is injective.
        
        \item \textbf{Surjective}: Suppose $(r_1, r_2, \dots, r_n) \in F^n$. Then we see $r_1\textbf{u}_1 + r_2\textbf{u}_2 + \cdots + r_n\textbf{u}_n \in V$, and
        \[
            T(r_1\textbf{u}_1 + r_2\textbf{u}_2 + \cdots + r_n\textbf{u}_n) = (r_1, r_2, \dots, r_n).
        \]
        Therefore any $(r_1, r_2, \dots, r_n) \in F^n$ has a pre-image in $V$, meaning $T$ is surjective.
    \end{itemize}
    Hence $T$ is a bijective linear transformation, which means $V$ and $F^n$ are isomorphic as vector spaces, as required.
\end{questions}
