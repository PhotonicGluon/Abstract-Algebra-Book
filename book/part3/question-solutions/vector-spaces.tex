\section{Vector Spaces}
\subsection*{Exercises}
\begin{questions}
    \item We first need to prove the four group axioms.
    \begin{itemize}
        \item \textbf{Closure}: We clearly see
        \[
            (a_1, a_2, \dots, a_n) + (b_1, b_2, \dots, b_n) = (a_1 + b_1, a_2 + b_2, \dots, a_n + b_n) \in \R^n
        \]
        so $\R^n$ is closed under vector addition.

        \item \textbf{Associativity}: Let $(a_1, a_2, \dots, a_n), (b_1, b_2, \dots, b_n), (c_1, c_2, \dots, c_n) \in \R^n$. Note
        \begin{align*}
            &(a_1, a_2, \dots, a_n) + \left((b_1, b_2, \dots, b_n) + (c_1, c_2, \dots, c_n)\right)\\
            &= (a_1, a_2, \dots, a_n) + (b_1 + c_1, b_2 + c_2, \dots, b_n + c_n)\\
            &= (a_1 + (b_1 + c_1), a_2 + (b_2 + c_2), \dots, a_n + (b_n + c_n))\\
            &= ((a_1 + b_1) + c_1, (a_2 + b_2) + c_2, \dots, (a_n + b_n) + c_n)\\
            &= (a_1 + b_1, a_2 + b_2, \dots, a_n + b_n) + (c_1, c_2, \dots, c_n)\\
            &= \left((a_1, a_2, \dots, a_n) + (b_1, b_2, \dots, b_n)\right) + (c_1, c_2, \dots, c_n)
        \end{align*}
        so vector addition is associative.

        \item \textbf{Identity}: One sees that $(0, 0, \dots, 0) \in \R^n$ is the identity.

        \item \textbf{Inverse}: One sees that $(-a_1, -a_2, \dots, -a_n) \in \R^n$ is the inverse of the element $(a_1, a_2, \dots, a_n)$.
    \end{itemize}

    Thus $(\R^n, +)$ is a group. Also, for any $(a_1, a_2, \dots, a_n), (b_1, b_2, \dots, b_n) \in \R^n$ we see
    \begin{align*}
        (a_1, a_2, \dots, a_n) + (b_1, b_2, \dots, b_n) &= (a_1 + b_1, a_2 + b_2, \dots, a_n + b_n)\\
        &= (b_1+a_1, b_2+a_2, \dots, b_n+a_n)\\
        &= (b_1, b_2, \dots, b_n) + (a_1, a_2, \dots, a_n)
    \end{align*}
    so vector addition is also commutative. Therefore $(\R^n, +)$ is an abelian group.

    \item No. Note $x^5 + 1 \in W$ and $-x^5 + 1 \in W$ but $(x^5+1) + (-x^5+1) = 2 \notin W$ since 2 has a degree of 0.

    \item Note
    \[
        \alpha\mathbf{0} = \alpha(\mathbf{0} + \mathbf{0}) = \alpha\mathbf{0} + \alpha\mathbf{0}
    \]
    by \textbf{Distributivity-Addition}. Then, by adding $-(\alpha\mathbf{0})$ on both sides we see
    \begin{align*}
        \alpha\mathbf{0} + (-(\alpha\mathbf{0})) &= (\alpha\mathbf{0} + \alpha\mathbf{0}) + (-(\alpha\mathbf{0}))\\
        &= \alpha\mathbf{0} + (\alpha\mathbf{0} + (-(\alpha\mathbf{0}))) & (\textbf{Addition-Abelian-Associativity})\\
    \end{align*}
    which means
    \[
        \mathbf{0} = \alpha\mathbf{0} + \mathbf{0},
    \]
    by \textbf{Addition-Abelian-Inverse}, and hence $\alpha\mathbf{0} = \mathbf{0}$, by \textbf{Addition-Abelian-Identity}.

    \item We know from \myref{example-polynomial-ring-over-field-is-vector-space} that $F[x]$ is a vector space, so we just need to prove that $V$ is a subspace of $F[x]$.
    \begin{itemize}
        \item Clearly $0 \in V$.
        \item Let $f(x), g(x) \in V$. We consider two cases.
        \begin{itemize}
            \item Suppose that $f(x)$ or $g(x)$ is the zero polynomial; without loss of generality assume $g(x) = 0$. Then $f(x) + g(x) = f(x) + 0 = f(x) \in V$.
            \item Otherwise we know that $\deg f(x), \deg g(x) \leq n$. By \myref{thrm-polynomial-degree-properties} we thus see that $\deg (f(x) + g(x)) \leq n$, meaning $f(x) + g(x) \in V$.
        \end{itemize}
        In either case, $f(x) + g(x) \in V$.
        \item Let $\alpha \in F$ and $f(x) \in V$. We again consider two cases.
        \begin{itemize}
            \item If $f(x) = 0$, then $\alpha f(x) = 0$ which is in $V$.
            \item Otherwise, we know that $\deg f(x) \leq n$ and multiplication by a constant does not change the degree. Hence $\deg (\alpha f(x)) \leq n$ and so $\alpha f(x) \in V$.
        \end{itemize}
        In either case, $\alpha f(x) \in V$.
    \end{itemize}
    Therefore, by the subspace test (\myref{thrm-subspace-test}), we see $V$ is a subspace of $F[x]$ over $F$, meaning that $V$ is a vector space over $F$.

    \item We show that $S$ is a spanning set of $V$.

    Let $a + bx + cx^2 \in V$. Suppose
    \[
        \alpha x + \beta(x + 2) + \gamma(x^2 + 2) + \delta(x^2 + 2x + 3) = a + bx + cx^2,
    \]
    where $\alpha, \beta, \gamma, \delta \in \Q$, i.e.
    \[
        (\gamma + \delta)x^2 + (\alpha + \beta + 2\delta)x + (2\beta + 2\gamma + 3\delta) = a + bx + cx^2.
    \]
    So we see
    \begin{align*}
        \gamma + \delta = a\\
        \alpha + \beta + 2\delta = b\\
        2\beta + 2\gamma + 3\delta = c
    \end{align*}
    which we can solve for $\alpha, \beta, \gamma, \delta$, yielding
    \begin{align*}
        \alpha &= a + b - \frac{c}{2} - \frac{3\delta}{2}\\
        \beta &= \frac{c}{2} - a - \frac{\delta}{2}\\
        \gamma &= a - \delta
    \end{align*}
    and $\delta$ is a free variable. Therefore any element of $V$ can be expressed as a linear combination of vectors in $S$, meaning $\Span{S} = V$.

    \item \begin{partquestions}{\alph*}
        \item Linearly independent since the only solution to
        \[
            \alpha + \beta(x+2) = 0,
        \]
        which yields the system of equations
        \begin{align*}
            \alpha + 2\beta &= 0,\\
            \beta &= 0,
        \end{align*}
        is the trivial solution $\alpha = \beta = 0$.

        \item Linearly independent since the only solution to
        \[
            \alpha + \beta(x+2) + \gamma(x^2 + 2x + 3) = 0,
        \]
        which yields the system of equations
        \begin{align*}
            \alpha + 2\beta + 3\gamma &= 0,\\
            \beta + 2\gamma &= 0,\\
            \gamma = 0
        \end{align*}
        is the trivial solution $\alpha = \beta = \gamma = 0$.

        \item Not linearly independent (i.e., linearly dependent) since
        \[
            1 - 2(x+2) + \frac12(x^2 + 2x + 3) - \frac12(x^2 - 2x - 3) = 0.
        \]
    \end{partquestions}

    \item Suppose $S = \{\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n\}$ is a basis for $V$. Suppose $\mathbf{u} \in V$ can be written as
    \[
        \mathbf{u} = \alpha_1\mathbf{v}_1 + \alpha_2\mathbf{v}_2 + \cdots + \alpha_n\mathbf{v}_n = \beta_1\mathbf{v}_1 + \beta_2\mathbf{v}_2 + \cdots + \beta_n\mathbf{v}_n
    \]
    where $\alpha_1, \alpha_2, \dots, \alpha_n, \beta_1, \beta_2, \dots, \beta_n \in F$. So we see
    \[
        (\alpha_1 - \beta_1)\mathbf{v}_1 + (\alpha_2 - \beta_2)\mathbf{v}_2 + \cdots + (\alpha_n - \beta_n)\mathbf{v}_n = \mathbf{0}.
    \]
    As $S$ is a basis, the vectors in $S$ are linearly independent. Therefore the only way for the sum to be the zero vector is for $\alpha_i - \beta_i = 0$ for all $i = 1, 2, \dots, n$, i.e. $\alpha_i = \beta_i$. Therefore any vector in $V$ can be expressed uniquely as a linear combination of the vectors in $S$.

    \item \begin{partquestions}{\alph*}
        \item Not a basis since $x^2$ cannot be expressed as a linear combination of the polynomials in $S$, i.e. $\Span{S} \neq V$.

        \item Is a basis. We found in \myref{exercise-polynomial-of-degree-at-most-2-vector-space} that the vectors in $T$ are linearly independent. Furthermore, any polynomial $ax^2 + bx + c \in V$ can be written as
        \[
            a(x^2+2x+3) + (-2a+b)(x+2) + (a-2b+c)
        \]
        which shows that $T$ spans $V$. Therefore $T$ is a basis for $V$.

        \item Not a basis as $U$ is not linearly independent (from \myref{exercise-polynomial-of-degree-at-most-2-vector-space}).
    \end{partquestions}

    \item We earlier found that $T = \{1, x + 2, x^2 + 2x + 3\}$ is a basis for $V$. Since $|T| = 3$ therefore $\dim{V} = 3$.
\end{questions}

\subsection*{Problems}
\begin{questions}
    \item \begin{partquestions}{\alph*}
        \item Since $\mathbf{0} \in U$ (as $U$ is a subspace of $V$) and since $\mathbf{0} \in W$ (as $W$ is a subspace of $V$), therefore $\mathbf{0} \in U \cap W$, which means $U \cap W$ is non-empty.

        Now suppose $\mathbf{u}, \mathbf{v} \in U \cap W$. So $\mathbf{u}, \mathbf{v} \in U$ and $\mathbf{u}, \mathbf{v} \in W$. Hence $\mathbf{u} + \mathbf{v} \in U$ and $\mathbf{u} + \mathbf{v} \in W$ by closure of vector spaces by addition. Therefore $\mathbf{u} + \mathbf{v} \in U \cap W$.

        Now let $\alpha \in F$ and $\mathbf{u} \in U \cap W$. So $\mathbf{u} \in U$ and $\mathbf{u} \in W$. This means $\alpha\mathbf{u} \in U$ and $\alpha\mathbf{u} \in W$ by closure of vector spaces by scalar multiplication. Hence $\alpha\mathbf{u} \in U \cap W$.

        So we see $U \cap W$ is a subspace of $V$ by the subspace test (\myref{thrm-subspace-test}).

        \item Note that since $\mathbf{0} \in U$ and $\mathbf{0} \in W$, we see $\mathbf{0} = \mathbf{0} + \mathbf{0} \in U + W$, meaning $U + W$ is non-empty.

        Now suppose $\mathbf{a}, \mathbf{b} \in U + W$, meaning $\mathbf{a} = \mathbf{u}_1 + \mathbf{w}_1$ and $\mathbf{b} = \mathbf{u}_2 + \mathbf{w}_2$ for some $\mathbf{u}_1, \mathbf{u}_2 \in U$ and $\mathbf{w}_1, \mathbf{w}_2 \in W$. Hence one sees $\mathbf{u}_1 + \mathbf{u}_2 \in U$ and $\mathbf{w}_1 + \mathbf{w}_2 \in W$, meaning
        \begin{align*}
            \mathbf{a} + \mathbf{b} &= (\mathbf{u}_1 + \mathbf{w}_1) + (\mathbf{u}_2 + \mathbf{w}_2)\\
            &= (\mathbf{u}_1 + \mathbf{u}_2) + (\mathbf{w}_1 + \mathbf{w}_2)\\
            &\in U + W.
        \end{align*}
        Also, for any $\alpha \in F$ we see $\alpha\mathbf{u}_1 \in U$ and $\alpha\mathbf{w}_1 \in W$, which means
        \begin{align*}
            \alpha\mathbf{a} &= \alpha(\mathbf{u}_1 + \mathbf{w}_1)\\
            &= (\alpha\mathbf{u}_1) + (\alpha\mathbf{w}_1)\\
            &\in U + W.
        \end{align*}
        Therefore $U + W$ is a subspace of $V$ by the subspace test (\myref{thrm-subspace-test}).
    \end{partquestions}

    \item Consider $S = \{\mathbf{u}_1, \mathbf{u}_2, \mathbf{u}_3\} = \{\mathbf{v}_1, \mathbf{v}_1 + \mathbf{v}_2, \mathbf{v}_1 + \mathbf{v}_2 + \mathbf{v}_3\}$. Since $\{\mathbf{v}_1,\mathbf{v}_2,\mathbf{v}_3\}$ is a basis for $V$, meaning that any $\mathbf{a} \in V$ can be expressed as
    \[
        \alpha_1\mathbf{v}_1 + \alpha_2\mathbf{v}_2 + \alpha_3\mathbf{v}_3
    \]
    where $\alpha_1, \alpha_2, \alpha_3 \in F$. One therefore sees
    \begin{align*}
        \mathbf{a} &= \alpha_1\mathbf{v}_1 + \alpha_2\mathbf{v}_2 + \alpha_3\mathbf{v}_3\\
        &= (\alpha_1 - \alpha_2)\mathbf{v}_1 + (\alpha_2 - \alpha_3)(\mathbf{v}_1 + \mathbf{v}_2) + \alpha_3(\mathbf{v}_1 + \mathbf{v}_2 + \mathbf{v}_3)\\
        &= (\alpha_1 - \alpha_2)\mathbf{u}_1 + (\alpha_2 - \alpha_3)\mathbf{u}_2 + \alpha_3\mathbf{u}_3
    \end{align*}
    which means $\Span{S} = V$.

    Now we show that the vectors in $S$ are linearly independent. Suppose that
    \[
        \beta_1\mathbf{u}_1 + \beta_2\mathbf{u}_2 + \beta_3\mathbf{u}_3 = \mathbf{0}
    \]
    for some $\beta_1, \beta_2, \beta_3 \in F$. This means
    \begin{align*}
        \beta_1\mathbf{u}_1 + \beta_2\mathbf{u}_2 + \beta_3\mathbf{u}_3 &= \beta_1\mathbf{v}_1 + \beta_2(\mathbf{v}_1 + \mathbf{v}_2) + \beta_3(\mathbf{v}_1 + \mathbf{v}_2 + \mathbf{v}_3)\\
        &= (\beta_1+\beta_2+\beta_3)\mathbf{v}_1 + (\beta_2+\beta_3)\mathbf{v}_2 + \beta_3\mathbf{v}_3\\
        &= \mathbf{0}.
    \end{align*}
    Since $\{\mathbf{v}_1, \mathbf{v}_2, \mathbf{v}_3\}$ is a basis, the only way for this to occur is when
    \begin{align*}
        \beta_1 + \beta_2 + \beta_3 &= 0,\\
        \beta_2 + \beta_3 &= 0, \text{ and}\\
        \beta_3 & = 0,
    \end{align*}
    which clearly means $\beta_1 = \beta_2 = \beta_3 = 0$. Therefore the vectors in $S$ are linearly independent, which shows that $S$ is a basis for $V$.

    \item \begin{partquestions}{\alph*}
        \item Note $(1, 0, 0, 0), (0, 1, 0, 0) \in R$ (since $1 + 0 + 0 + 0 = 1$ and $0 + 1 + 0 + 0 = 1$) but
        \[
            (1, 0, 0, 0) + (0, 1, 0, 0) = (1, 1, 0, 0)
        \]
        is not in $R$ since $1 + 1 + 0 + 0 = 2 \neq 1$. Therefore $R$ is not closed under vector addition, meaning that it is not a vector space (and hence not a subspace).

        \item We first show that $S$ is a subspace of $V$.

        Note $(0, 0, 0, 0) \in S$ since $0 + 0 + 0 + 0 = 0$, which means $S$ is non-empty.

        Now suppose $(a_1, a_2, a_3, a_4), (b_1, b_2, b_3, b_4) \in V$. So
        \begin{align*}
            a_1 + a_2 + a_3 + a_4 &= 0\\
            b_1 + b_2 + b_3 + b_4 &= 0
        \end{align*}
        which therefore means
        \begin{align*}
            &(a_1 + b_1) + (a_2 + b_2) + (a_3 + b_3) + (a_4 + b_4)\\
            &= (a_1 + a_2 + a_3 + a_4) + (b_1 + b_2 + b_3 + b_4)\\
            &= 0.
        \end{align*}
        Hence
        \[
            (a_1, a_2, a_3, a_4) + (b_1, b_2, b_3, b_4) = (a_1 + b_1, a_2 + b_2, a_3 + b_3, a_4 + b_4) \in S.
        \]

        Finally, suppose $\alpha \in \R$ and $(a_1, a_2, a_3, a_4) \in V$. One sees that
        \[
            \alpha a_1 + \alpha a_2 + \alpha a_3 + \alpha a_4 = \alpha(a_1 + a_2 + a_3 + a_4) = 0
        \]
        and so
        \[
            \alpha(a_1, a_2, a_3, a_4) = (\alpha a_1, \alpha a_2 , \alpha a_3, \alpha a_4) \in S.
        \]

        Therefore, by the subspace test (\myref{thrm-subspace-test}), we see $S$ is a subspace of $V$.

        Now note that any $(a, b, c, d) \in S$ is expressible in the form
        \[
            (a, b, c, -a-b-c) = a(1, 0, 0, -1) + b(0, 1, 0, -1) + c(0, 0, 1, -1)
        \]
        and so the vectors in the set $A = \{(1, 0, 0, -1), (0, 1, 0, -1), (0, 0, 1, -1)\}$ span $V$. We also see that if
        \[
            \alpha(1, 0, 0, -1) + \beta(0, 1, 0, -1) + \gamma(0, 0, 1, -1) = (0, 0, 0, 0)
        \]
        for some $\alpha, \beta, \gamma \in F$, then we must have
        \begin{align*}
            \alpha &= 0\\
            \beta &= 0\\
            \gamma &= 0\\
            -\alpha - \beta - \gamma &= 0
        \end{align*}
        and so the only solution to the above equation is the trivial one, where $\alpha = \beta = \gamma = 0$. This shows that the vectors in $A$ are linearly independent.

        Therefore $A$ is a basis for $S$. Since $|A| = 3$, therefore $S$ has dimension 3, i.e. $\dim{S} = 3$.
    \end{partquestions}

    \item We are to prove the vector space axioms for $F^n$ over $F$.
    \begin{itemize}
        \item \textbf{Addition-Abelian}: We show that $(F^n, +)$ is an abelian group.
        \begin{itemize}
            \item \textbf{Closure}: From the definition of vector addition it is clear that closure is satisfied.

            \item \textbf{Associativity}: For any $(u_1, u_2, \dots, u_n), (v_1, v_2, \dots, v_n), (w_1, w_2, \dots, w_n) \in F^n$ we see
            \begin{align*}
                &(u_1, u_2, \dots, u_n) + ((v_1, v_2, \dots, v_n) + (w_1, w_2, \dots, w_n))\\
                &= (u_1, u_2, \dots, u_n) + (v_1 + w_1, v_2 + w_2, \dots, v_n + w_n)\\
                &= (u_1 + (v_1 + w_1), u_2 + (v_2 + w_2), \dots, u_n + (v_n + w_n))\\
                &= ((u_1 + v_1) + w_1, (u_2 + v_2) + w_2, \dots, (u_n + v_n) + w_n)\\
                &= (u_1 + v_1, u_2 + v_2, \dots, u_n + v_n) + (w_1, w_2, \dots, w_n)\\
                &= ((u_1, u_2, \dots, u_n) + (v_1, v_2, \dots, v_n)) + (w_1, w_2, \dots, w_n)
            \end{align*}
            which proves that vector addition is associative.

            \item \textbf{Identity}: $(0, 0, \dots, 0) \in F^n$ is the identity since, for any $(u_1, u_2, \dots, u_n) \in F^n$, we see
            \begin{align*}
                (0, 0, \dots, 0) + (u_1, u_2, \dots, u_n) &= (0 + u_1, 0 + u_2, \dots, 0 + u_n)\\
                &= (u_1, u_2, \dots, u_n).
            \end{align*}

            \item \textbf{Inverse}: For any $(u_1, u_2, \dots, u_n) \in F^n$, we note that $(-u_1, -u_2, \dots, -u_n)$ is its additive inverse since
            \begin{align*}
                (u_1, u_2, \dots, u_n) + (-u_1, -u_2, \dots, -u_n) &= (u_1 - u_1, u_2 - u_2, \dots, u_n - u_n)\\
                &= (0, 0, \dots, 0).
            \end{align*}

            \item \textbf{Commutativity}: For any $(u_1, u_2, \dots, u_n), (v_1, v_2, \dots, v_n) \in F^n$ we note
            \begin{align*}
                (u_1, u_2, \dots, u_n) + (v_1, v_2, \dots, v_n) &= (u_1 + v_1, u_2 + v_2, \dots, u_n + v_n)\\
                &= (v_1 + u_1, v_2 + u_2, \dots, v_n + u_n)\\
                &= (v_1, v_2, \dots, v_n) + (u_1, u_2, \dots, u_n)
            \end{align*}
            so addition is commutative.
        \end{itemize}
        Therefore $(F^n, +)$ is an abelian group.

        \item \textbf{Multiplication-Identity}: Note for any $(u_1, u_2, \dots, u_n) \in F^n$ we have
        \begin{align*}
            1(u_1, u_2, \dots, u_n) &= (1u_1, 1u_2, \dots, 1u_n)\\
            &= (u_1, u_2, \dots, u_n)
        \end{align*}
        so this axiom is satisfied.

        \item \textbf{Multiplication-Compatibility}: Let $\alpha, \beta \in F$ and $(u_1, u_2, \dots, u_n) \in F^n$. Then
        \begin{align*}
            \alpha(\beta(u_1, u_2, \dots, u_n)) &= \alpha(\beta u_1, \beta u_2, \dots, \beta u_n)\\
            &= (\alpha\beta u_1, \alpha\beta u_2, \dots, \alpha\beta u_n)\\
            &= (\alpha\beta)(u_1, u_2, \dots, u_n),
        \end{align*}
        showing that this axiom is satisfied.

        \item \textbf{Distributivity-Addition}: Let $\alpha \in F$ and $(u_1, u_2, \dots, u_n), (v_1, v_2, \dots, v_n) \in F^n$. Then
        \begin{align*}
            &\alpha((u_1, u_2, \dots, u_n) + (v_1, v_2, \dots, v_n))\\
            &= \alpha(u_1 + v_1, u_2 + v_2, \dots, u_n + v_n)\\
            &= (\alpha(u_1 + v_1), \alpha(u_2 + v_2), \dots, \alpha(u_n + v_n))\\
            &= (\alpha u_1 + \alpha v_1, \alpha u_2 + \alpha v_2, \dots, \alpha u_n + \alpha v_n)\\
            &= (\alpha u_1, \alpha u_2, \dots, \alpha u_n) + (\alpha v_1, \alpha v_2, \dots, \alpha v_n)\\
            &= \alpha(u_1, u_2, \dots, u_n) + \alpha(v_1, v_2, \dots, v_n)
        \end{align*}
        which shows that the axiom is satisfied.

        \item \textbf{Distributivity-Scalar}: Let $\alpha, \beta \in F$ and $(u_1, u_2, \dots, u_n) \in F^n$. Note
        \begin{align*}
            (\alpha+\beta)(u_1, u_2, \dots, u_n) &= ((\alpha+\beta)u_1, (\alpha+\beta)u_2, \dots, (\alpha+\beta)u_n)\\
            &= (\alpha u_1 + \beta u_1, \alpha u_2 + \beta u_2, \dots, \alpha u_n + \beta u_n)\\
            &= (\alpha u_1, \alpha u_2, \dots, \alpha u_n) + (\beta u_1, \beta u_2, \dots, \beta u_n)\\
            &= \alpha(u_1, u_2, \dots, u_n) + \beta(u_1, u_2, \dots, u_n)
        \end{align*}
        so this axiom is satisfied.
    \end{itemize}
    Since all the vector space axioms are satisfied, this finally means that $F^n$ is a vector space over $F$.

    \item Suppose $V$ is a vector space over the field $F$ with a basis of $\{\mathbf{u}_1, \mathbf{u}_2, \dots, \mathbf{u}_n\}$. This means any $\mathbf{a} \in V$ can be expressed as
    \[
        \alpha_1\mathbf{u}_1 + \alpha_2\mathbf{u}_2 + \cdots + \alpha_n\mathbf{u}_n
    \]
    where $\alpha_1,\alpha_2,\dots,\alpha_n \in F$.

    Consider the map $T: V \to F^n$ given by
    \[
        T( \alpha_1\mathbf{u}_1 + \alpha_2\mathbf{u}_2 + \cdots + \alpha_n\mathbf{u}_n) = (\alpha_1, \alpha_2, \dots, \alpha_n).
    \]
    We first show that $T$ is a linear transformation.
    \begin{itemize}
        \item For all $\alpha_1\mathbf{u}_1 + \alpha_2\mathbf{u}_2 + \cdots + \alpha_n\mathbf{u}_n, \beta_1\mathbf{u}_1 + \beta_2\mathbf{u}_2 + \cdots + \beta_n\mathbf{u}_n \in V$ we see
        \begin{align*}
            &T((\alpha_1\mathbf{u}_1 + \alpha_2\mathbf{u}_2 + \cdots + \alpha_n\mathbf{u}_n) + (\beta_1\mathbf{u}_1 + \beta_2\mathbf{u}_2 + \cdots + \beta_n\mathbf{u}_n))\\
            &= T((\alpha_1 + \beta_1)\mathbf{u}_1 + (\alpha_2 + \beta_2)\mathbf{u}_2 + \cdots + (\alpha_n + \beta_n)\mathbf{u}_n)\\
            &= (\alpha_1 + \beta_1, \alpha_2 + \beta_2, \dots, \alpha_n + \beta_n)\\
            &= (\alpha_1, \alpha_2, \dots, \alpha_n) + (\beta_1, \beta_2, \dots, \beta_n)\\
            &= T(\alpha_1\mathbf{u}_1 + \alpha_2\mathbf{u}_2 + \cdots + \alpha_n\mathbf{u}_n) + T(\beta_1\mathbf{u}_1 + \beta_2\mathbf{u}_2 + \cdots + \beta_n\mathbf{u}_n).
        \end{align*}

        \item For all $\lambda \in F$ and $\alpha_1\mathbf{u}_1 + \alpha_2\mathbf{u}_2 + \cdots + \alpha_n\mathbf{u}_n \in V$ we see
        \begin{align*}
            T(\lambda(\alpha_1\mathbf{u}_1 + \alpha_2\mathbf{u}_2 + \cdots + \alpha_n\mathbf{u}_n)) &= T((\lambda\alpha_1)\mathbf{u}_1 + (\lambda\alpha_2)\mathbf{u}_2 + \cdots + (\lambda\alpha_n)\mathbf{u}_n)\\
            &= (\lambda\alpha_1, \lambda\alpha_2, \dots, \lambda\alpha_n)\\
            &= \lambda(\alpha_1, \alpha_2, \dots, \alpha_n)\\
            &= \lambda T(\alpha_1\mathbf{u}_1 + \alpha_2\mathbf{u}_2 + \cdots + \alpha_n\mathbf{u}_n).
        \end{align*}
    \end{itemize}
    Therefore $T$ is a linear transformation.

    We now prove that $T$ is a bijection.
    \begin{itemize}
        \item \textbf{Injective}: Suppose $\alpha_1\mathbf{u}_1 + \alpha_2\mathbf{u}_2 + \cdots + \alpha_n\mathbf{u}_n, \beta_1\mathbf{u}_1 + \beta_2\mathbf{u}_2 + \cdots + \beta_n\mathbf{u}_n \in V$ are such that $T(\alpha_1\mathbf{u}_1 + \alpha_2\mathbf{u}_2 + \cdots + \alpha_n\mathbf{u}_n) = T(\beta_1\mathbf{u}_1 + \beta_2\mathbf{u}_2 + \cdots + \beta_n\mathbf{u}_n)$. This therefore means $(\alpha_1, \alpha_2, \dots, \alpha_n) = (\beta_1, \beta_2, \dots, \beta_n)$, which shows that $\alpha_i = \beta_i$ for all $i \in \{1, 2, \dots, n\}$. It follows quickly from this result that $\alpha_1\mathbf{u}_1 + \alpha_2\mathbf{u}_2 + \cdots + \alpha_n\mathbf{u}_n = \beta_1\mathbf{u}_1 + \beta_2\mathbf{u}_2 + \cdots + \beta_n\mathbf{u}_n$, proving that $T$ is injective.

        \item \textbf{Surjective}: Suppose $(r_1, r_2, \dots, r_n) \in F^n$. Then we see $r_1\mathbf{u}_1 + r_2\mathbf{u}_2 + \cdots + r_n\mathbf{u}_n \in V$, and
        \[
            T(r_1\mathbf{u}_1 + r_2\mathbf{u}_2 + \cdots + r_n\mathbf{u}_n) = (r_1, r_2, \dots, r_n).
        \]
        Therefore any $(r_1, r_2, \dots, r_n) \in F^n$ has a pre-image in $V$, meaning $T$ is surjective.
    \end{itemize}
    Hence $T$ is a bijective linear transformation, which means $V$ and $F^n$ are isomorphic as vector spaces, as required.
\end{questions}
